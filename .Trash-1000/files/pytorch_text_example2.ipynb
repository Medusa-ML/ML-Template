{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "0c5f49c0-422c-4353-b9c4-5cfd32286864",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[33mWARNING: The directory '/.cache/pip' or its parent directory is not owned or is not writable by the current user. The cache has been disabled. Check the permissions and owner of that directory. If executing pip with sudo, you should use sudo's -H flag.\u001b[0m\u001b[33m\n",
      "\u001b[0mDefaulting to user installation because normal site-packages is not writeable\n",
      "Collecting torchtext\n",
      "  Obtaining dependency information for torchtext from https://files.pythonhosted.org/packages/1a/4b/40c40574e7f76cfea6b6b94928bb7d6ca44bf5aa1869347d8a71d7ff0563/torchtext-0.16.0-cp310-cp310-manylinux1_x86_64.whl.metadata\n",
      "  Downloading torchtext-0.16.0-cp310-cp310-manylinux1_x86_64.whl.metadata (7.5 kB)\n",
      "Requirement already satisfied: tqdm in /opt/conda/lib/python3.10/site-packages (from torchtext) (4.65.0)\n",
      "Requirement already satisfied: requests in /opt/conda/lib/python3.10/site-packages (from torchtext) (2.31.0)\n",
      "Requirement already satisfied: torch==2.1.0 in /opt/conda/lib/python3.10/site-packages (from torchtext) (2.1.0)\n",
      "Requirement already satisfied: numpy in /opt/conda/lib/python3.10/site-packages (from torchtext) (1.26.0)\n",
      "Collecting torchdata==0.7.0 (from torchtext)\n",
      "  Obtaining dependency information for torchdata==0.7.0 from https://files.pythonhosted.org/packages/58/3f/e805df66f0308eebf735f794e87164013024924efac22b4432d7c09374ea/torchdata-0.7.0-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata\n",
      "  Downloading torchdata-0.7.0-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (13 kB)\n",
      "Requirement already satisfied: filelock in /opt/conda/lib/python3.10/site-packages (from torch==2.1.0->torchtext) (3.9.0)\n",
      "Requirement already satisfied: typing-extensions in /opt/conda/lib/python3.10/site-packages (from torch==2.1.0->torchtext) (4.7.1)\n",
      "Requirement already satisfied: sympy in /opt/conda/lib/python3.10/site-packages (from torch==2.1.0->torchtext) (1.11.1)\n",
      "Requirement already satisfied: networkx in /opt/conda/lib/python3.10/site-packages (from torch==2.1.0->torchtext) (3.1)\n",
      "Requirement already satisfied: jinja2 in /opt/conda/lib/python3.10/site-packages (from torch==2.1.0->torchtext) (3.1.2)\n",
      "Requirement already satisfied: fsspec in /opt/conda/lib/python3.10/site-packages (from torch==2.1.0->torchtext) (2023.9.2)\n",
      "Requirement already satisfied: urllib3>=1.25 in /opt/conda/lib/python3.10/site-packages (from torchdata==0.7.0->torchtext) (1.26.16)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in /opt/conda/lib/python3.10/site-packages (from requests->torchtext) (2.0.4)\n",
      "Requirement already satisfied: idna<4,>=2.5 in /opt/conda/lib/python3.10/site-packages (from requests->torchtext) (3.4)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /opt/conda/lib/python3.10/site-packages (from requests->torchtext) (2023.7.22)\n",
      "Requirement already satisfied: MarkupSafe>=2.0 in /opt/conda/lib/python3.10/site-packages (from jinja2->torch==2.1.0->torchtext) (2.1.1)\n",
      "Requirement already satisfied: mpmath>=0.19 in /opt/conda/lib/python3.10/site-packages (from sympy->torch==2.1.0->torchtext) (1.3.0)\n",
      "Downloading torchtext-0.16.0-cp310-cp310-manylinux1_x86_64.whl (2.0 MB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m2.0/2.0 MB\u001b[0m \u001b[31m32.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0ma \u001b[36m0:00:01\u001b[0m\n",
      "\u001b[?25hDownloading torchdata-0.7.0-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (4.7 MB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m4.7/4.7 MB\u001b[0m \u001b[31m42.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0ma \u001b[36m0:00:01\u001b[0m\n",
      "\u001b[?25hInstalling collected packages: torchdata, torchtext\n",
      "Successfully installed torchdata-0.7.0 torchtext-0.16.0\n"
     ]
    }
   ],
   "source": [
    "! pip install torchtext"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "bb029366-4068-4ea3-8581-7ac3d8950dbd",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from collections import Counter\n",
    "from torchtext.vocab import Vocab\n",
    "from torch.utils.data import DataLoader, Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "704f6da6-0729-4a32-82ca-57076bee0c98",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Enhanced Binary Text Classifier\n",
    "class EnhancedBinaryTextClassifier(nn.Module):\n",
    "    def __init__(self, vocab_size, embedding_dim, hidden_dim):\n",
    "        super(EnhancedBinaryTextClassifier, self).__init__()\n",
    "        self.embedding = nn.Embedding(vocab_size, embedding_dim)\n",
    "        self.dropout = nn.Dropout(p=0.5)\n",
    "        self.fc1 = nn.Linear(embedding_dim, hidden_dim)\n",
    "        self.fc2 = nn.Linear(hidden_dim, 1)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.embedding(x)\n",
    "        x = torch.mean(x, dim=1)\n",
    "        x = self.dropout(x)\n",
    "        x = torch.relu(self.fc1(x))\n",
    "        x = self.fc2(x)\n",
    "        return torch.sigmoid(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "e53e1776-8f31-4127-8a6b-d27afe448732",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Data Preparation\n",
    "def preprocess(text):\n",
    "    return text.lower().split()\n",
    "\n",
    "def build_vocab(sentences):\n",
    "    counter = Counter()\n",
    "    for sentence in sentences:\n",
    "        counter.update(preprocess(sentence))\n",
    "    return Vocab(counter)\n",
    "\n",
    "def encode(sentence, vocab):\n",
    "    return [vocab.stoi[word] for word in preprocess(sentence)]\n",
    "\n",
    "# Sample Statements\n",
    "republican_statements = [\n",
    "    \"We must prioritize national security and strong borders.\",\n",
    "    \"Lowering taxes is essential for economic growth and prosperity.\",\n",
    "    \"It's crucial to defend the Second Amendment rights.\",\n",
    "    \"Small government and individual freedoms are the core of our policy.\",\n",
    "    \"Fiscal responsibility and balanced budgets should be our goal.\"\n",
    "]\n",
    "\n",
    "democratic_statements = [\n",
    "    \"Healthcare should be accessible and affordable for everyone.\",\n",
    "    \"We need to address climate change with urgent environmental policies.\",\n",
    "    \"Education funding is vital for the future of our country.\",\n",
    "    \"We stand for social justice and equality for all citizens.\",\n",
    "    \"Investing in renewable energy is key for a sustainable future.\"\n",
    "]\n",
    "\n",
    "all_statements = republican_statements + democratic_statements\n",
    "labels = [0] * len(republican_statements) + [1] * len(democratic_statements)  # 0 for Republican, 1 for Democrat"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "47f14b7b-c6ad-4f0e-9bda-eaabdbd50279",
   "metadata": {},
   "outputs": [
    {
     "ename": "AttributeError",
     "evalue": "'Vocab' object has no attribute 'stoi'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[6], line 5\u001b[0m\n\u001b[1;32m      2\u001b[0m vocab \u001b[38;5;241m=\u001b[39m build_vocab(all_statements)\n\u001b[1;32m      4\u001b[0m \u001b[38;5;66;03m# Encode Statements\u001b[39;00m\n\u001b[0;32m----> 5\u001b[0m encoded_data \u001b[38;5;241m=\u001b[39m [(encode(sentence, vocab), label) \u001b[38;5;28;01mfor\u001b[39;00m sentence, label \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mzip\u001b[39m(all_statements, labels)]\n\u001b[1;32m      7\u001b[0m \u001b[38;5;66;03m# Dataset and DataLoader\u001b[39;00m\n\u001b[1;32m      8\u001b[0m \u001b[38;5;28;01mclass\u001b[39;00m \u001b[38;5;21;01mTextDataset\u001b[39;00m(Dataset):\n",
      "Cell \u001b[0;32mIn[6], line 5\u001b[0m, in \u001b[0;36m<listcomp>\u001b[0;34m(.0)\u001b[0m\n\u001b[1;32m      2\u001b[0m vocab \u001b[38;5;241m=\u001b[39m build_vocab(all_statements)\n\u001b[1;32m      4\u001b[0m \u001b[38;5;66;03m# Encode Statements\u001b[39;00m\n\u001b[0;32m----> 5\u001b[0m encoded_data \u001b[38;5;241m=\u001b[39m [(\u001b[43mencode\u001b[49m\u001b[43m(\u001b[49m\u001b[43msentence\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mvocab\u001b[49m\u001b[43m)\u001b[49m, label) \u001b[38;5;28;01mfor\u001b[39;00m sentence, label \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mzip\u001b[39m(all_statements, labels)]\n\u001b[1;32m      7\u001b[0m \u001b[38;5;66;03m# Dataset and DataLoader\u001b[39;00m\n\u001b[1;32m      8\u001b[0m \u001b[38;5;28;01mclass\u001b[39;00m \u001b[38;5;21;01mTextDataset\u001b[39;00m(Dataset):\n",
      "Cell \u001b[0;32mIn[5], line 12\u001b[0m, in \u001b[0;36mencode\u001b[0;34m(sentence, vocab)\u001b[0m\n\u001b[1;32m     11\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mencode\u001b[39m(sentence, vocab):\n\u001b[0;32m---> 12\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m [vocab\u001b[38;5;241m.\u001b[39mstoi[word] \u001b[38;5;28;01mfor\u001b[39;00m word \u001b[38;5;129;01min\u001b[39;00m preprocess(sentence)]\n",
      "Cell \u001b[0;32mIn[5], line 12\u001b[0m, in \u001b[0;36m<listcomp>\u001b[0;34m(.0)\u001b[0m\n\u001b[1;32m     11\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mencode\u001b[39m(sentence, vocab):\n\u001b[0;32m---> 12\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m [\u001b[43mvocab\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mstoi\u001b[49m[word] \u001b[38;5;28;01mfor\u001b[39;00m word \u001b[38;5;129;01min\u001b[39;00m preprocess(sentence)]\n",
      "File \u001b[0;32m~opt/conda/lib/python3.10/site-packages/torch/nn/modules/module.py:1695\u001b[0m, in \u001b[0;36mModule.__getattr__\u001b[0;34m(self, name)\u001b[0m\n\u001b[1;32m   1693\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m name \u001b[38;5;129;01min\u001b[39;00m modules:\n\u001b[1;32m   1694\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m modules[name]\n\u001b[0;32m-> 1695\u001b[0m \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mAttributeError\u001b[39;00m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[38;5;28mtype\u001b[39m(\u001b[38;5;28mself\u001b[39m)\u001b[38;5;241m.\u001b[39m\u001b[38;5;18m__name__\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m object has no attribute \u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mname\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n",
      "\u001b[0;31mAttributeError\u001b[0m: 'Vocab' object has no attribute 'stoi'"
     ]
    }
   ],
   "source": [
    "# Build Vocabulary\n",
    "vocab = build_vocab(all_statements)\n",
    "\n",
    "# Encode Statements\n",
    "encoded_data = [(encode(sentence, vocab), label) for sentence, label in zip(all_statements, labels)]\n",
    "\n",
    "# Dataset and DataLoader\n",
    "class TextDataset(Dataset):\n",
    "    def __init__(self, data):\n",
    "        self.data = data\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.data)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        return self.data[idx]\n",
    "\n",
    "dataset = TextDataset(encoded_data)\n",
    "dataloader = DataLoader(dataset, batch_size=1, shuffle=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "60451b19-4953-4d5a-bec5-c6410b234236",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Model Initialization\n",
    "model = EnhancedBinaryTextClassifier(len(vocab), embedding_dim=10, hidden_dim=20)\n",
    "loss_function = nn.BCELoss()\n",
    "optimizer = optim.SGD(model.parameters(), lr=0.01)\n",
    "\n",
    "# Training Loop\n",
    "def train_model(model, epochs, dataloader):\n",
    "    for epoch in range(epochs):\n",
    "        total_loss = 0\n",
    "        for inputs, target in dataloader:\n",
    "            inputs = inputs[0]  # Unwrap batch\n",
    "            model.zero_grad()\n",
    "            outputs = model(inputs)\n",
    "            loss = loss_function(outputs, target.float())\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "            total_loss += loss.item()\n",
    "        print(f'Epoch {epoch + 1}/{epochs}, Loss: {total_loss:.4f}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3cde1885-270c-4ac4-9f76-6528ac25444f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Training the Model\n",
    "train_model(model, epochs=10, dataloader=dataloader)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
